# Responsible AI: Ethical Boundaries in Text-to-Image Generation Models

This project presents a responsible AI pipeline that moderates content in text-to-image generation using **NSFW content detection** at both the text and image levels. It incorporates **RoBERTa** for text classification, **Stable Diffusion** for image generation, and **Vision Transformer (ViT)** for image classification, ensuring ethical boundaries are respected throughout.

---

## ğŸ“Œ Objectives

- Detect and block NSFW prompts before image generation.
- Reduce explicit or harmful content generated by AI models.
- Implement a post-generation NSFW image detection filter.
- Provide explainability and transparency in AI decisions.

---

## ğŸ§  System Pipeline

```scss
Text Prompt â†’ [NSFW Text Classifier (RoBERTa)] --(IF Safe)-â†’ [Stable Diffusion] â†’ [NSFW Image Classifier (ViT)] --(IF Safe)-â†’ Show Image Generated
```

---

## ğŸ—‚ï¸ Project Structure

```yaml
Responsible-AI-Ethical-Boundaries-in-Text-to-Image-Generation-Model/
â”‚
â”œâ”€â”€ assets/ # Image warning assets
â”‚ â”œâ”€â”€ ImageBlocked.png
â”‚ â””â”€â”€ PromptBlocked.png
â”‚
â”œâ”€â”€ Fine Tuning Pre-Trained RoBERTa Model/
â”‚ â”œâ”€â”€ Classified Synthetic Dataset.csv # Custom NSFW/Safe dataset
â”‚ â””â”€â”€ NSFW_Text_Classification_Training.ipynb
â”‚
â”œâ”€â”€ Fine Tuning Pre-Trained ViT Model/
â”‚ â””â”€â”€ NSFW_Image_Classification_Training.ipynb
â”‚
â”œâ”€â”€ main.py # Gradio UI + NSFW text detection
â”œâ”€â”€ NSFWImageDetection.py # NSFW image classification logic (ViT)
â”œâ”€â”€ run.ipynb # One-liner to launch Gradio UI
â”œâ”€â”€ requirements.txt # All dependencies
â””â”€â”€ README.md # This file
```

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/AimanZaharin/Responsible-AI-Ethical-Boundaries-in-Text-to-Image-Generation-Model
cd Responsible-AI-Ethical-Boundaries-in-Text-to-Image-Generation-Model
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Run the Application

You can run this application in two ways:

#### ğŸ§ª Option 1: Run via Jupyter Notebook

Open and run the run.ipynb file locally to launch the Gradio UI.

```bash
jupyter notebook run.ipynb
```

#### ğŸ’» Option 2: Run via Command Line

Run the app directly using Python (ensure dependencies are installed):

```bash
python main.py
```
A Gradio interface will launch at localhost, allowing you to test the pipeline interactively in your browser.

---

## ğŸ› ï¸ Model Fine-Tuning Process

This project involves **head-only fine-tuning** of two pre-trained transformer models to detect NSFW content effectively in both text and image formats.

---

### ğŸ”¤ Fine-Tuning RoBERTa for NSFW Text Classification

- **Goal:** Adapt a pre-trained RoBERTa model to detect unsafe or suggestive prompts using a labeled NSFW/Safe dataset.

- **Location:**  
  `Fine Tuning Pre-Trained RoBERTa Model/NSFW_Text_Classification_Training.ipynb`

- **Dataset:**  
  `Classified Synthetic Dataset.csv` â€” a curated dataset with "NSFW" and "Safe" labels.

- **Training Highlights:**
  - Only the final classification head is fine-tuned.
  - Trained on synthetic examples designed to resemble real-world prompts.
  - Evaluated for high precision on sensitive content.
  - Outputs a three class classification: `Safe`, `Questionable`, or `Unsafe`.

---

### ğŸ–¼ï¸ Fine-Tuning ViT for NSFW Image Classification

- **Goal:** Fine-tune a pre-trained Vision Transformer (ViT) model to detect unsafe or explicit visual content.

- **Location:**  
  `Fine Tuning Pre-Trained ViT Model/NSFW_Image_Classification_Training.ipynb`

- **Dataset:**  
  A synthetic NSFW image dataset (not publicly included due to sensitivity).

- **Training Highlights:**
  - Similar head-only fine-tuning strategy.
  - Trained to distinguish safe from unsafe generated images post-generation.
  - Outputs a binary classification: `Safe` or `NSFW`.

---

Both models are exported and uploaded to **Hugging Face**, ready for reuse or inference:

- [RoBERTa NSFW Text Classifier](https://huggingface.co/AimanZaharin/RoBERTa-based-NSFW-Text-Detection)
- [ViT NSFW Image Classifier](https://huggingface.co/AimanZaharin/ViT-NSFW-Image-Detection)

---

## ğŸ’¡ Features

- âœ… Head-only fine-tuning on RoBERTa for custom NSFW text classification
- ğŸ§  Fine-tuned a Pre-trained ViT model for NSFW image detection
- ğŸ–¼ï¸ Seamless integration with Stable Diffusion for image generation
- ğŸ’¬ Gradio-based user interface for real-time testing
- ğŸ”’ Ethical filtering at two stages: text and image

---

## ğŸ“Š Sample Results

The system handles three moderation scenarios based on prompt and image content:

---

### âœ… 1. Safe Prompt â†’ Image Generated

- **Prompt:**  
  `A cat sitting on a windowsill during a rainy day`

- **Outcome:**  
  This prompt is classified as **safe** by the RoBERTa-based NSFW text classifier.  
  Image is successfully generated by the Stable Diffusion model and will be displayed.

- **Generated Image:**  
  ![Safe Image Example](assets/safeDemo.png)

---

### âš ï¸ 2. Questionable/Unsafe Prompt â†’ Prompt Blocked

- **Prompt:**  
  `A person in a highly revealing or suggestive outfit in a provocative pose`

- **Outcome:**  
  This prompt is classified as **NSFW** or **QUESTIONABLE** at the **text level** by the RoBERTa-based classifier.  
  Image generation is **blocked**, and the user sees a warning screen instead.

- **Warning Image:**  
  ![Prompt Blocked](assets/unsafeQuestDemo.png)

---

### âŒ 3. Safe Prompt â†’ NSFW Image Generated â†’ Image Blocked

- **Prompt:**  
  `A woman stranded on a deserted island with her clothes wet having nothing but leaves to cover herself`

- **Outcome:**  
  The prompt passes the text filter but results in an **NSFW image**.  
  The ViT-based NSFW image classifier blocks the visual output post-generation.

- **Warning Image:**  
  ![Image Blocked](assets/imageBlockedDemo.png)

---

This three-tier moderation system ensures ethical filtering at both **text** and **image** levels for responsible AI usage.


---

## âš–ï¸ Ethical Considerations

This project embodies **Responsible AI** practices:

- **Safety:** Dual-stage NSFW filters help prevent harmful outputs.
- **Openness:** The full pipeline is accessible and modifiable.
